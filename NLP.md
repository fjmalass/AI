# NLP

## Transformers

- Bert

## Embeddings

1. Dense representation of information
2. Examples

- Word2Vec [1 word = same embedding]
- Glove
- Latent Semanitc Analysis (LDA)
- Latent Dirichlet Allocation (LDA)
- Biterm Topic Model (BTM)
- Inception (Image processing) [Neural Network]
- BERT (Bidirectional Encoder Representations of Transformers)
- Use of Approximate Nearest Neighbor (ANN) [Like for spotify]

3. Implementation

- Redis

## Libraries

-[`SpaCy`](https://spacy.io)
